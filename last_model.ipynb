{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle\n",
    "import scipy.io\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import Sequential\n",
    "from scipy.signal import periodogram\n",
    "from scipy.signal import firwin, lfilter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_length = 600\n",
    "overlap = 300\n",
    "cutoff_freq = [1, 15]\n",
    "num_taps = 50\n",
    "h = firwin(num_taps, cutoff_freq, fs=100, pass_zero=False)\n",
    "n_components = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/albk/Documents/Code/Real_Projects/MicroSleep/Datas/Track#2 Microsleep detection from single-channel EEG/Training set\"\n",
    "\n",
    "for n in range(1, 51):\n",
    "  if n < 10:\n",
    "    with open(root + f\"/Data_Sample0{n}.mat\", \"rb\") as g:\n",
    "      globals()[f\"df{n}\"] = scipy.io.loadmat(g)\n",
    "  else:\n",
    "    with open(root + f\"/Data_Sample{n}.mat\", \"rb\") as r:\n",
    "      globals()[f\"df{n}\"] = scipy.io.loadmat(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Data sort\n",
    "drowsy_freq = []\n",
    "awake_freq = []\n",
    "drowsy_inds = {}\n",
    "awake_inds = {}\n",
    "\n",
    "# this loop iterates over labels and get index of each 0 and 1 class to awake_inds and drowsy_inds lists \n",
    "for d in range(1, 51):\n",
    "  drowsy_inds[f\"df{d}\"] = []\n",
    "  awake_inds[f\"df{d}\"] = []\n",
    "\n",
    "  for ind, label in enumerate(globals()[f\"df{d}\"][\"epo\"][0][0][1][0]):\n",
    "    if label == 1:\n",
    "      drowsy_inds[f\"df{d}\"].append(ind)\n",
    "    elif label == 0:\n",
    "      awake_inds[f\"df{d}\"].append(ind)\n",
    "\n",
    "# after sorting indexes of classes, we'll sort and save each 3000-sample window by their classes to drowsy and awake lists\n",
    "  for lb1, dr in enumerate(globals()[f\"df{d}\"][\"epo\"][0][0][0]):\n",
    "    if lb1 in drowsy_inds[f\"df{d}\"]:\n",
    "      # dr = lfilter(h, 1, dr)\n",
    "      windows = librosa.util.frame(dr, frame_length=frame_length, hop_length=overlap).transpose(1, 0)\n",
    "      han_windows = np.multiply(windows, np.hanning(frame_length))\n",
    "      feats = []\n",
    "      for win in han_windows:\n",
    "        feats.append(np.array(np.abs(np.array(rfft(win)[10:180])).tolist()))\n",
    "        # feats.append(mfccs_feats(win, 100))\n",
    "\n",
    "      drowsy_freq.append(feats)\n",
    "\n",
    "    if len(awake_freq) != 6482:\n",
    "      if lb1 in awake_inds[f\"df{d}\"]:\n",
    "        # dr = lfilter(h, 1, dr)\n",
    "        windows = librosa.util.frame(dr, frame_length=frame_length, hop_length=overlap).transpose(1, 0)\n",
    "        han_windows = np.multiply(windows, np.hanning(frame_length))\n",
    "        feats = []        \n",
    "        for win in han_windows:\n",
    "          feats.append(np.array(np.abs(np.array(rfft(win)[10:180])).tolist()))\n",
    "          # feats.append(mfccs_feats(win, 100))\n",
    "        \n",
    "        awake_freq.append(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components).fit(np.vstack([awake_freq, drowsy_freq]).reshape((len(awake_freq) + len(drowsy_freq)) * len(awake_freq[0]), len(awake_freq[0][0])))\n",
    "\n",
    "awake_freq = pca.transform(np.real(np.array(awake_freq).reshape(len(awake_freq) * len(awake_freq[0]), len(awake_freq[0][0])))).reshape(len(awake_freq), len(awake_freq[0]), n_components).tolist()\n",
    "drowsy_freq = pca.transform(np.real(np.array(drowsy_freq).reshape(len(drowsy_freq) * len(drowsy_freq[0]), len(drowsy_freq[0][0])))).reshape(len(drowsy_freq), len(drowsy_freq[0]), n_components).tolist()\n",
    "\n",
    "scaler = StandardScaler().fit(np.real(np.array(awake_freq + drowsy_freq).reshape((len(awake_freq) + len(drowsy_freq)) * len(awake_freq[0]), len(awake_freq[0][0]))))\n",
    "\n",
    "awake_freq = scaler.transform(np.real(np.array(awake_freq).reshape(len(awake_freq) * len(awake_freq[0]), len(awake_freq[0][0])))).reshape(len(awake_freq), len(awake_freq[0]), len(awake_freq[0][0]))\n",
    "drowsy_freq = scaler.transform(np.real(np.array(drowsy_freq).reshape(len(drowsy_freq) * len(drowsy_freq[0]), len(drowsy_freq[0][0])))).reshape(len(drowsy_freq), len(drowsy_freq[0]), len(drowsy_freq[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "d_label = [1] * (drowsy_freq.shape[0])\n",
    "a_label = [0] * (awake_freq.shape[0])\n",
    "\n",
    "# Zip the features and labels\n",
    "data_d = list(zip(drowsy_freq, d_label))\n",
    "data_a = list(zip(awake_freq, a_label))\n",
    "data = data_a + data_d\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Unzip the data\n",
    "features, labels = zip(*data)\n",
    "\n",
    "features = np.array(features, dtype=np.float32)\n",
    "labels = np.array(labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_set loading\n",
    "v_root = \"/Users/albk/Documents/Code/Real_Projects/MicroSleep/Datas/Track#2 Microsleep detection from single-channel EEG/Validation set\"\n",
    "for n in range(1, 11):\n",
    "  if n < 10:\n",
    "    with open(v_root + f\"/Data_Sample0{n}.mat\", \"rb\") as g:\n",
    "      globals()[f\"df{n}\"] = scipy.io.loadmat(g)\n",
    "  else:\n",
    "    with open(v_root + f\"/Data_Sample{n}.mat\", \"rb\") as r:\n",
    "      globals()[f\"df{n}\"] = scipy.io.loadmat(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation Data sort\n",
    "v_drowsy_freq = []\n",
    "v_awake_freq = []\n",
    "drowsy_inds = {}\n",
    "awake_inds = {}\n",
    "\n",
    "# this loop iterates over labels and get index of each 0 and 1 class to awake_inds and drowsy_inds lists \n",
    "for d in range(1, 11):\n",
    "  drowsy_inds[f\"df{d}\"] = []\n",
    "  awake_inds[f\"df{d}\"] = []\n",
    "\n",
    "  for ind, label in enumerate(globals()[f\"df{d}\"][\"epo\"][0][0][2][0]):\n",
    "    if label == 1:\n",
    "      drowsy_inds[f\"df{d}\"].append(ind)\n",
    "    elif label == 0:\n",
    "      awake_inds[f\"df{d}\"].append(ind)\n",
    "\n",
    "# after sorting indexes of classes, we'll sort and save each 3000-sample window by their classes to drowsy and awake lists\n",
    "  for lb1, dr in enumerate(globals()[f\"df{d}\"][\"epo\"][0][0][1]):\n",
    "    if lb1 in drowsy_inds[f\"df{d}\"]:\n",
    "      # dr = lfilter(h, 1, dr)\n",
    "      windows = librosa.util.frame(dr, frame_length=frame_length, hop_length=overlap).transpose(1, 0)\n",
    "      han_windows = np.multiply(windows, np.hanning(frame_length))\n",
    "      feats = []\n",
    "      for win in han_windows:\n",
    "        feats.append(np.array(np.abs(np.array(rfft(win)[10:180])).tolist()))\n",
    "        # feats.append(mfccs_feats(win, 100))\n",
    "\n",
    "      v_drowsy_freq.append(feats)\n",
    "\n",
    "    if lb1 in awake_inds[f\"df{d}\"]:\n",
    "      # dr = lfilter(h, 1, dr)\n",
    "      windows = librosa.util.frame(dr, frame_length=frame_length, hop_length=overlap).transpose(1, 0)\n",
    "      han_windows = np.multiply(windows, np.hanning(frame_length))\n",
    "      feats = []\n",
    "      for win in han_windows:\n",
    "        feats.append(np.array(np.abs(np.array(rfft(win)[10:180])).tolist()))\n",
    "        # feats.append(mfccs_feats(win, 100))\n",
    "\n",
    "      v_awake_freq.append(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_awake_freq = pca.transform(np.real(np.array(v_awake_freq).reshape(len(v_awake_freq) * len(v_awake_freq[0]), len(v_awake_freq[0][0])))).reshape(len(v_awake_freq), len(v_awake_freq[0]), n_components).tolist()\n",
    "v_drowsy_freq = pca.transform(np.real(np.array(v_drowsy_freq).reshape(len(v_drowsy_freq) * len(v_drowsy_freq[0]), len(v_drowsy_freq[0][0])))).reshape(len(v_drowsy_freq), len(v_drowsy_freq[0]), n_components).tolist()\n",
    "v_awake_freq = scaler.transform(np.real(v_awake_freq).reshape(len(v_awake_freq) * len(v_awake_freq[0]), len(v_awake_freq[0][0]))).reshape(len(v_awake_freq), len(v_awake_freq[0]), len(v_awake_freq[0][0]))\n",
    "v_drowsy_freq = scaler.transform(np.real(v_drowsy_freq).reshape(len(v_drowsy_freq) * len(v_drowsy_freq[0]), len(v_drowsy_freq[0][0]))).reshape(len(v_drowsy_freq), len(v_drowsy_freq[0]), len(v_drowsy_freq[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_d_label = [1] * (v_drowsy_freq.shape[0])\n",
    "v_a_label = [0] * (v_awake_freq.shape[0])\n",
    "\n",
    "# Zip the features and labels\n",
    "v_data_d = list(zip(v_drowsy_freq, v_d_label))\n",
    "v_data_a = list(zip(v_awake_freq, v_a_label))\n",
    "v_data = v_data_a + v_data_d\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(v_data)\n",
    "\n",
    "# Unzip the data\n",
    "v_features, v_labels = zip(*v_data)\n",
    "\n",
    "v_features = np.array(v_features, dtype=np.float16)\n",
    "v_labels = np.array(v_labels, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12964, 9, 7) (12964,) (1214, 9, 7) (1214,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape, labels.shape, v_features.shape, v_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "input_shape = (features.shape[1], features.shape[2])\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"elu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def TransformerModel(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Reshape((input_shape[0], input_shape[1], 1))(inputs)\n",
    "    x = layers.Conv2D(filters=32, kernel_size=(3,3), activation='elu')(x)\n",
    "    x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "    x = layers.Conv2D(filters=128, kernel_size=(3,1), activation='elu')(x)\n",
    "    x = layers.MaxPool2D(pool_size=(1,2))(x)\n",
    "    # x = layers.Conv2D(filters=32, kernel_size=(1,2), activation='elu')(x)\n",
    "    # x = layers.MaxPool2D(pool_size=(1,2))(x)\n",
    "    # x = layers.BatchNormalization()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.Activation('elu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.Activation('elu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.Activation('elu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Reshape((1, x.shape[-1]))(x)\n",
    "    x = TransformerEncoder(embed_dim=128, num_heads=4, ff_dim=128)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = TransformerModel(input_shape, num_classes)\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(clipnorm=1),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "404/406 [============================>.] - ETA: 0s - loss: 0.3570 - accuracy: 0.8411\n",
      "Epoch 1: val_accuracy improved from -inf to 0.53460, saving model to /Users/albk/Documents/Code/Real_Projects/MicroSleep/model/best_model.h5\n",
      "406/406 [==============================] - 5s 10ms/step - loss: 0.3565 - accuracy: 0.8413 - val_loss: 0.9574 - val_accuracy: 0.5346\n",
      "Epoch 2/10\n",
      "403/406 [============================>.] - ETA: 0s - loss: 0.2687 - accuracy: 0.8906\n",
      "Epoch 2: val_accuracy did not improve from 0.53460\n",
      "406/406 [==============================] - 3s 7ms/step - loss: 0.2683 - accuracy: 0.8909 - val_loss: 1.2752 - val_accuracy: 0.4984\n",
      "Epoch 3/10\n",
      "402/406 [============================>.] - ETA: 0s - loss: 0.2336 - accuracy: 0.9066\n",
      "Epoch 3: val_accuracy improved from 0.53460 to 0.54119, saving model to /Users/albk/Documents/Code/Real_Projects/MicroSleep/model/best_model.h5\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 0.2336 - accuracy: 0.9066 - val_loss: 1.1516 - val_accuracy: 0.5412\n",
      "Epoch 4/10\n",
      "402/406 [============================>.] - ETA: 0s - loss: 0.2139 - accuracy: 0.9136\n",
      "Epoch 4: val_accuracy did not improve from 0.54119\n",
      "406/406 [==============================] - 3s 7ms/step - loss: 0.2136 - accuracy: 0.9135 - val_loss: 1.2455 - val_accuracy: 0.5231\n",
      "Epoch 5/10\n",
      "399/406 [============================>.] - ETA: 0s - loss: 0.2014 - accuracy: 0.9205\n",
      "Epoch 5: val_accuracy improved from 0.54119 to 0.55189, saving model to /Users/albk/Documents/Code/Real_Projects/MicroSleep/model/best_model.h5\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 0.2017 - accuracy: 0.9202 - val_loss: 1.0786 - val_accuracy: 0.5519\n",
      "Epoch 6/10\n",
      "402/406 [============================>.] - ETA: 0s - loss: 0.1958 - accuracy: 0.9216\n",
      "Epoch 6: val_accuracy did not improve from 0.55189\n",
      "406/406 [==============================] - 3s 7ms/step - loss: 0.1957 - accuracy: 0.9217 - val_loss: 1.6328 - val_accuracy: 0.5494\n",
      "Epoch 7/10\n",
      "403/406 [============================>.] - ETA: 0s - loss: 0.1850 - accuracy: 0.9276\n",
      "Epoch 7: val_accuracy improved from 0.55189 to 0.57990, saving model to /Users/albk/Documents/Code/Real_Projects/MicroSleep/model/best_model.h5\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 0.1853 - accuracy: 0.9274 - val_loss: 1.0741 - val_accuracy: 0.5799\n",
      "Epoch 8/10\n",
      "402/406 [============================>.] - ETA: 0s - loss: 0.1730 - accuracy: 0.9318\n",
      "Epoch 8: val_accuracy improved from 0.57990 to 0.62685, saving model to /Users/albk/Documents/Code/Real_Projects/MicroSleep/model/best_model.h5\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 0.1730 - accuracy: 0.9317 - val_loss: 1.0451 - val_accuracy: 0.6269\n",
      "Epoch 9/10\n",
      "405/406 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9319\n",
      "Epoch 9: val_accuracy did not improve from 0.62685\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 0.1716 - accuracy: 0.9319 - val_loss: 1.3087 - val_accuracy: 0.5684\n",
      "Epoch 10/10\n",
      "404/406 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9348\n",
      "Epoch 10: val_accuracy did not improve from 0.62685\n",
      "406/406 [==============================] - 3s 8ms/step - loss: 0.1689 - accuracy: 0.9349 - val_loss: 1.2164 - val_accuracy: 0.5700\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model_root = \"/Users/albk/Documents/Code/Real_Projects/MicroSleep/model\"\n",
    "checkpoint = ModelCheckpoint(model_root + \"/best_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "# early_stop = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', verbose=1)\n",
    "history = model.fit(features, labels, epochs=10, validation_data=(v_features, v_labels), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: 'TransformerEncoder'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model\n\u001b[1;32m      3\u001b[0m \u001b[39m# Load the best model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m best_model \u001b[39m=\u001b[39m load_model(model_root \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/best_model.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Evaluate the best model on the test set\u001b[39;00m\n\u001b[1;32m      7\u001b[0m test_loss, test_acc \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39mevaluate(v_features, v_labels)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/saving/saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[1;32m    205\u001b[0m         filepath,\n\u001b[1;32m    206\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[1;32m    207\u001b[0m         \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m,\n\u001b[1;32m    208\u001b[0m         safe_mode\u001b[39m=\u001b[39msafe_mode,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m    213\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39;49mcustom_objects, \u001b[39mcompile\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcompile\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    214\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/saving/legacy/serialization.py:368\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m object_registration\u001b[39m.\u001b[39mget_registered_object(\n\u001b[1;32m    365\u001b[0m     class_name, custom_objects, module_objects\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    369\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown \u001b[39m\u001b[39m{\u001b[39;00mprintable_module_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mclass_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure you are using a `keras.utils.custom_object_scope` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand that this object is included in the scope. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#registering_the_custom_object for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m cls_config \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    377\u001b[0m \u001b[39m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[39m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[39m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: 'TransformerEncoder'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the best model\n",
    "best_model = load_model(model_root + \"/best_model.h5\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_acc = best_model.evaluate(v_features, v_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
